1.交叉验证 (Cross-Validation):
    交叉验证是一种用于评估机器学习模型性能和可靠性的统计方法。
    它的核心思想是将原始数据集反复地分割成训练集（Training Set）和测试集（Testing Set），然后用训练集来训练模型，用测试集来评估模型。
k折交叉验证（k-Fold Cross-Validation）:
    确定k值：将一个常数k（通常是5或10）作为折数（Folds）。
    分割数据集：将原始数据集随机打乱并平均分成k个大小相似的互斥子集，称为“折”（Fold）。
    进行k次循环：
    在每一次循环中，将1个折作为测试集，剩下的 k-1个折合并起来作为训练集。
    用训练集训练模型。
    用测试集评估模型，得到一个性能评分（如准确率、精确率等）。
    汇总结果：k次循环结束后，你会得到k个模型性能评分。最终模型的性能评估结果是这k个评分的平均值。
    这个过程确保了每个子集都有且仅有一次被用作测试集。



2.混淆矩阵（Confusion Matrix）：
    混淆矩阵是一个特定的表格布局，用于可视化一个分类模型的性能。
    它将模型的预测结果与数据的真实标签进行对比，并汇总成四个核心类别，从而让我们能够清楚地看到模型在哪些地方做对了，在哪些地方混淆了
                         真实值为 Positive (1)         真实值为 Negative (0)
预测为 Positive (1)    TP (True Positive，真正例)    FP (False Positive，假正例)
预测为 Negative (0)    FN (False Negative，假负例)   TN (True Negative，真负例)

准确率 (Accuracy)：
    所有预测中，猜对的比例
    (TP + TN) / (TP + TN + FP + FN)
精确率 (Precision)：
    在所有预测为“正”的案例中，有多少是预测正确的。衡量的是模型的“准不准”：
    TP / (TP + FP)
召回率 (Recall)，又称敏感性(Sensitivity)：
    在所有真实为“正”的案例中，模型成功预测出了多少。衡量的是模型的“全不全”，有没有漏掉：
    TP / (TP + FN)
特异性（Specificity）：
    在所有真实为“负”的案例中，模型成功预测出了多少。
    TN / (TN + FP)
如果你想评估模型找出所有特定类别（如“苹果”）的能力，就看该类的敏感性。
如果你想评估模型避免将其他东西误认为特定类别的能力，就看该类的特异性。



3.偏差 (Bias)：
    定义：偏差衡量了模型的平均预测结果与真实值之间的差距。即模型本身的拟合能力有多强。
    高偏差：模型过于简单，无法捕捉数据中的潜在规律（欠拟合）。无论给多少数据，它的平均预测都和真实值差得很远。
    低偏差：模型复杂度足够，平均预测结果很接近真实值。

方差 (Variance)：
    定义：方差衡量了模型对于不同训练数据集的敏感程度。即模型的稳定性有多高。
    高方差：模型过于复杂，完美地学习了训练数据中的每一个细节，包括噪声。导致换一份数据训练，模型的表现差异就非常大（过拟合）。
    低方差：模型很简单，换不同的数据训练，模型的变化不大，表现很稳定。



4.熵 (Entropy)：
    衡量一个系统的不确定性、混乱程度或者惊喜度。不确定性越高，熵越大。确定性越高，熵越小。
    计算方式见：书签（熵的计算）



5.决定系数(R-squared)：
    用于衡量回归模型（如线性回归）的拟合优度（Goodness of Fit）。
    R² = ESS / TSS = 1 - (SSE / SST)
    模型的功劳 / 总的问题的规模。这个比值越大，说明模型的功劳占比越大，模型就越好。|从“误差剩余”的角度出发。
    一个回归模型所解释的因变量（目标变量）的方差占其总方差的比例。
    它的值域在 0 到 1 之间（有时也会用百分比 0% 到 100% 表示）。
    R² = 0：意味着你的模型完全无法解释因变量的任何波动。模型是无效的。
    R² = 1：意味着你的模型完美地解释了因变量的所有波动。所有数据点都恰好落在回归线上。
    通常，R² 的值介于两者之间，值越高，表明模型的自变量对因变量的解释能力越强，模型的拟合效果越好。

yᵢ是每个数据点的实际值。
ȳ 是所有实际值的平均值。
ŷᵢ是模型对每个数据点的预测值。

SST (Total Sum of Squares) - 总平方和
    SST = Σ(yᵢ - ȳ)²
    SST 衡量的是数据点围绕均值分布的分散程度。

SSR (Regression Sum of Squares) - 回归平方和
    SSR = Σ(ŷᵢ - ȳ)²
    表示由于模型的影响而带来的波动。它衡量的是预测值围绕均值的分散程度。这个分散是由回归线（模型）造成的，所以是模型解释掉的部分。

SSE (Error Sum of Squares) - 残差平方和（又名RSS(Residual Sum of Squares)）
    SSE = Σ(yᵢ - ŷᵢ)²
    表示模型无法解释的波动，即误差。它衡量的是数据点距离回归线的远近。距离越远，误差越大。

只要你向模型中增加新的自变量，无论这个变量是否与因变量真正相关，R-squared 的值都会保持不变或增加（几乎总是增加）。
即使加入一个随机变量，模型也总能“抓到”一点数据中的随机噪声，从而稍微减少一点SSE（误差）
根据公式 R² = 1 - (SSE / SST)，SST不变，SSE减小，R² 就会增大。
总波动 (TSS) = 模型解释掉的波动 (ESS) + 模型无法解释的波动 (RSS)
Σ(yᵢ - ȳ)² = Σ(ŷᵢ - ȳ)² + Σ(yᵢ - ŷᵢ)²
一个理想的模型，其目标就是最大化自己的功劳 (ESS)，同时最小化自己的失误 (RSS)。



6.联合概率 (joint probability)：
    联合概率指的是多个事件同时发生的概率。它衡量的是两个或更多事件交集的可能性。
    P(A, B) 或 P(A ∩ B) = 事件 A 和 事件 B 同时发生的概率。
边际概率(marginal probability)：
    边际概率是在联合概率分布中，只关心某一个事件的发生概率，而忽略其他事件的所有可能情况
    P(A) = 事件 A 发生的概率（无论 B 发生与否）
         = 所有包含 A 的联合概率之和
         = P(A, B₁) + P(A, B₂) + P(A, B₃) + ... (对所有可能的 B 求和)


7.互信息 (Mutual Information, MI)：
    互信息是信息论中的一个核心概念，用于衡量两个随机变量之间的相互依赖程度。
    更具体地说，它回答的问题是：“我知道一个变量X的信息后，能减少另一个变量Y的不确定性多少？”
I(X; Y) = Σₓ Σᵧ P(x, y) * log( P(x, y) / (P(x)P(y)) )
    计算方式见：书签（互信息的计算）
    核心比值：P(x, y) / (P(x)P(y))
    这个比值揭示了X和Y的真实关系与独立假设之间的偏离程度：
    如果比值 = 1：
    P(x, y) = P(x)P(y)
    这意味着对于这对具体的 (x, y)，X和Y是独立的。知道x的发生，对你猜测y的发生没有任何帮助，反之亦然。它们之间没有共享信息。
    如果比值 > 1：
    P(x, y) > P(x)P(y)
    这意味着x和y同时出现的概率比随机巧合要高得多。它们倾向于一起发生。知道x发生了，会极大地提高你预测y也会发生的信心。
    如果比值 < 1：
    P(x, y) < P(x)P(y)
    这意味着x和y同时出现的概率比随机巧合要低。它们倾向于互斥。知道x发生了，会极大地降低你预测y也会发生的信心。


8.简单线性回归(Simple Linear Regression)；
    是一种用于研究自变量（independent variable）与因变量（dependent variable）之间线性关系的统计方法，其核心是通过建立线性数学模型，
    来描述自变量的变化如何影响因变量的变化，并基于样本数据估计模型参数，最终用于预测或解释变量间的关系。
普通最小二乘法(Ordinary Least Squares,OLS)：
    找到一条直线，使得所有数据点到这条直线的垂直距离（称为“残差”或“误差”）的平方和最小
    即使RSS = Σ(yᵢ - ŷᵢ)²最小
均方（MS）= 平方和（Sum of Squares，简称SS）/自由度（Degrees of Freedom，简称df）
    平方和（SS）：反映数据与某个参考值（如均值）之间的总偏差平方和，衡量了数据的总变异程度。
    自由度（df）：反映计算平方和时独立数据点的数量（或自由变化的参数数量），用于校正平方和的偏差。

多元回归（Multiple Regression）：
    多元回归是简单线性回归的扩展，它用于研究一个因变量（Dependent Variable） 和两个或两个以上自变量（Independent Variables） 之间的关系。
        因变量 (Y)： 你想要预测或解释的结果。它也被称为“响应变量”。
            例如：房屋售价、员工工作效率、患者康复程度
        自变量 (X₁, X₂, X₃, ...)： 你认为可能影响因变量的因素。它们也被称为“预测变量”或“解释变量”。
            例如：房屋面积、卧室数量、房龄；员工的教育水平、工作经验；患者的年龄、治疗方案、体重


9.F 统计量：
    F统计量是用于检验整个回归模型整体显著性的指标，即是否至少有一个自变量对因变量有显著解释力。
    公式见：书签(F统计量公式)
    F=MSR/MSE
     =(SSR/df_R)/(SSE/df_E)
     =(模型解释的方差/自由度)/(残差方差/自由度)
     =(SSR/p)/(SSE/(n-p-1))

理解一：
MSR (Mean Square Due to Regression | 回归均方):
    SSR (Sum of Squares Due to Regression | 回归平方和):衡量的是完整模型比零模型多解释了多少变异。
        SSR = Σ(ŷᵢ - ȳ)²
    df_R (Regression Degrees of Freedom | 回归自由度)：等于模型中自变量的个数，通常记为 p，用来预测或解释因变量（Y）的预测变量的数量（如面积x1，房屋数量x2），这个数量不包括截距项 (β₀)。
        例如，模型 y = β₀ + β₁x₁ + β₂x₂ 中有2个自变量，所以 p = 2，df_R = 2。
MSR = SSR / df_R：代表了每个自变量平均能解释的变异。

MSE (Mean Square Due to Error | 残差均方):
    SSE (Sum of Squares Due to Error | 残差平方和)：衡量的是模型未能解释的剩余变异。
        SSE = Σ(yᵢ - ŷᵢ)²
    df_E (Error Degrees of Freedom | 残差自由度)：等于样本量减去模型等待估计的参数（模型参数总个数）。
        模型参数总个数 = 自变量个数 (p) + 1（截距项β₀）。
        df_E = n - p - 1，其中 n 是样本量。
MSE = SSE / df_E：代表了排除模型影响后，数据中剩余的、未被解释的平均变异。

分子：我的模型有多好？模型能解释多少变异（组间差异、解释平方和）。“平均每个自变量带来的解释力”
分母：我的模型有多差？模型没解释掉的变异（组内差异、残差平方和）。“平均每个剩余自由度上未解释的变异（噪音）”
F检验问的问题是：“我增加的这 p 个自变量，是否显著地比零模型更好地预测了数据？”
如果模型真的有用，那么分子会显著大于分母 ⇒ F 值大。
如果模型没用，那么分子 ≈ 分母 ⇒ F 值接近 1。

理解二：
SS_mean：使用“平均值模型”时的残差平方和。Σ(yᵢ - ȳ)²        SST（总平方和）
“平均值模型”就是最简单的模型：y = β₀ + ε。它只有一个参数（β₀，即均值），所以其参数数量 p_mean = 1。
这个模型的预测值对于所有数据点都是同一个值 ȳ（平均值）。
这个值衡量了数据本身的总波动性。

SS_fit：使用“拟合模型”后的残差平方和。Σ(yᵢ - ŷᵢ)²          SSE（残差平方和）
“拟合模型”就是你的回归方程，例如一条直线 y = β₀ + β₁x + ε。它有两个参数（β₀ 和 β₁），所以其参数数量 p_fit = 2。
这个模型的预测值 ŷ 是因变量在自变量条件下的条件均值。
这个值衡量了拟合模型后仍然无法解释的剩余波动。

F统计量衡量的是：从“平均值模型”到“拟合模型”，每个新增参数所能解释的波动比例的提升，相对于模型剩余的平均无法解释的波动，有多大。
a.模型改善了多少？
从只用均值(p=1)到使用拟合模型(p=2)，平方和减少了 (SS_mean - SS_fit)。
这减少的部分，就是回归模型所解释的波动，即 SSR：SSR = SS_mean - SS_fit。
b.为这个改善付出了多少代价？
代价是我们引入了更多的参数。新增参数的数量是 (p_fit - p_mean) = 2 - 1 = 1。
所以，每个新增参数平均带来的改善是：(SST - SSE) / (p_fit - p_mean)
c.改善的“性价比”高吗？
我们需要一个基准来衡量这个改善是否显著。这个基准就是拟合后每个数据点平均承担的无法解释的波动，即 SS_fit / (n - p_fit)。
这里的 (n - p_fit) 是残差的自由度（样本量减去拟合模型的参数总数）。
最终，F统计量就是这个“性价比”的比值：
F = 每个新增参数带来的平均改善/剩余的平均无法解释的波动

原假设 (H₀)：β₁ = β₂ = ... = βₙ = 0 （所有自变量对预测因变量都没有任何作用）
备择假设 (H₁)：至少有一个βᵢ ≠ 0 （至少有一个自变量对预测因变量是有用的）
F统计量服从F分布，这个分布由两个自由度参数决定（分子自由度和分母自由度）。
在原假设H₀为真的前提下，

10.P值（P-Value） 是在假设原假设（Null Hypothesis, H₀）为真时（无关），观察到当前样本数据（或更极端数据）的概率。
比如：
H₀：各组均值相等 / 回归模型系数全为 0。
计算出一个 F 值，查对应的 F 分布，得到在 H₀ 成立下得到 ≥ 该 F 值的概率。
这个概率就是 p-value。
F统计量是“检验强度”的度量（值越大，越可能拒绝零假设）。
p-value 是“拒绝零假设的证据大小”（概率越小，越有理由拒绝）。
举例：
对于一个双因素ANOVA（方差分析Analysis of Variance），得到 F = 5.2，自由度为 (3, 40)。
查 F 分布表，或者用 Python scipy.stats.f.sf(5.2, 3, 40)计算。
得到 p ≈ 0.004。
说明：在零假设为真时，得到一个 F ≥ 5.2 的概率只有 0.4%，很小 → 拒绝零假设。即有关。


11. t检验（t-test）：
    t检验是一种统计方法，用于比较两个组的平均值，判断它们之间的差异是否不仅仅是由于随机巧合造成的，而是具有统计学上的显著差异。
    它主要回答一个问题：“这两个组的平均值看起来不一样，但这个‘不一样’是真实存在的，还是只是因为运气好/运气差而偶然出现的？”
    t = 均值差/标准误差
    有三种：单样本 t-test、独立样本 t-test、配对 t-test
    对于独立样本：均值差 = 两组均值的差值； 标准误差（Standard Error, SE） = 详见4.SE计算公式
    t-test 和 F-test 在两组情况下是完全等价的：
    𝐹 = t**2

12.方差分析(ANOVA，全称 Analysis of Variance):
    是一种常用的统计方法，用来比较三组及以上样本均值之间是否存在显著差异。
    把数据中“总体差异”分解成两部分：
        组间差异（不同组的均值差异造成的变动）
        组内差异（同一组内个体之间的随机波动）
        然后比较这两部分的大小。如果“组间差异”远大于“组内差异”，就说明不同组的均值可能真的不同，而不仅仅是随机波动导致的。
    举例:
        假设你有三个班级的考试成绩，想知道这三个班的平均成绩是否有显著差异。
        如果三个班的均值差别很大，而班内学生分数差不多（组内差异小），那就说明班级之间确实存在差异。
        如果三个班的均值差不大，而班内分数差异很大（组内差异大），那就很难说班级均值不同。
    ANOVA 的统计量:
    用 F统计量 来判断显著性：
        F = 组间方差/组内方差
        如果F很大，就倾向于拒绝原假设（即“所有组的均值相同”）。
    通过 p-value 来决定是否显著（常用阈值 0.05）。
    常见的 ANOVA 类型：
        单因素方差分析（One-way ANOVA）：研究一个分类变量（因素）对均值的影响。(不同班的成绩和智商的关系)
        双因素方差分析（Two-way ANOVA）：研究两个因素对均值的影响，还可以考虑交互作用。(不同班的成绩和智商,睡眠的关系)
        重复测量方差分析（Repeated Measures ANOVA）：同一组样本在不同条件下重复测量时使用。


13.设计矩阵（Design Matrix）：
    也称为模型矩阵（Model Matrix） 或回归量矩阵（Regressor Matrix），是一个将数据集中所有观测值的所有自变量（特征）以矩阵形式组织起来的表格。
    它的核心作用是作为线性模型 Y = Xβ + ε 中的 X。
    一个典型的设计矩阵 X 的结构如下：
        行 (Rows)： 每一行代表一个观测样本（一个数据点）。
        列 (Columns)： 每一列代表一个变量（一个特征），包括：
            1.截距项 (Intercept Term)： 几乎总是第一列，所有值都是 1。
            2.自变量 (Independent Variables)： 原始的自变量。
            3.衍生变量： 如多项式项、交互项等。

    例：
    原数据：
    房屋ID    价格 (Y)    面积 (X₁)   卧室数 (X₂)    区域 (Location)
    1	       100	      50	         1              North
    2	       150	      75	         2              South
    3	       200	     100	         3              North
    location: 所在区域（分类变量），取值有两种："North" 和 "South"，用独热编码。
    我们想建立的多元线性回归模型是：
    Price = β₀ + β₁ * Size + β₂ * Bedrooms + β₃ * Location_North + ε
    设计矩阵 X 为：
            截距 (β₀) 面积 (β₁)	卧室数 (β₂)	Location_North (β₃)
    房屋1	   1	  50	       1	            1
    房屋2	   1	  75	       2	            0
    房屋3	   1	  100	       3	            1

    X =  [ 1 50  1 1 ]      系数向量 β 为：β = [ β₀ ]    向量 Y（因变量）为：Y = [ 100 ]
         [ 1 75  2 0 ]                       [ β₁ ]                          [ 150 ]
         [ 1 100 3 1 ]                       [ β₂ ]                          [ 200 ]        # 实际都为大括号
                                             [ β₃ ]
    则 Y = Xβ + ε 可转化为对应的代数形式


14.独热编码（One-Hot Encoding）：
    一种方法将分类数据（Categorical Data） 转换为数值数据（Numerical Data），同时又不能引入错误的数学关系。
    核心思想是用一组二进制位（0和1）来表示一个类别的存在与否。如果一个变量有 k 个类别，则创建 k-1 个新的二进制（0/1）列。
例：
    步骤一：对分类列“城市”进行独热编码
        1.识别类别：“城市”列有3个独一无二的类别：北京， 上海， 广州。
        2.创建新列：我们创建3个新列：城市_北京, 城市_上海, 城市_广州。
        3.填充数据：根据每一行原始“城市”列的值，在新列中填上1或0。
    步骤二：拼接回原表格并删除原始列
        现在，我们把编码后的新列和原来的数值列拼接在一起。同时，我们会丢弃原始的“城市”文本列，因为它的信息已经被新的三列完全取代了。
        为了避免“虚拟变量陷阱（k 个新特征列存在完全多重共线性，会导致模型计算问题）”，我们主动舍弃一列，比如舍弃 城市_广州 列，让“广州”作为参考基线。
    得到：
    价格(万)	面积(㎡)	卧室数(个)	城市_北京	    城市_上海
    100	     50	       1	        1	       0
    150	     75	       2	        0	       1
    200    	100 	   3	        0	       0
    120	     60	       1	        1	       0
    即为设计矩阵X
    价格 = β₀ + β₁ * 面积 + β₂ * 卧室数 + β₃ * 城市_北京 + β₄ * 城市_上海 + ε
    β₃ 的解释：在面积和卧室数相同的情况下，“北京”的房子比“广州”（基线）的房子平均贵 β₃ 万元。
    β₄ 的解释：在面积和卧室数相同的情况下，“上海”的房子比“广州”（基线）的房子平均贵 β₄ 万元。


15.几率 (Odds)：
    事件发生的概率与事件不发生的概率之比，范围是 [0, +∞)。
    计算公式：
        Odds = p / (1 - p)
    例：
        如果某事件发生的概率是 p = 0.75，那么：
        Odds = 0.75 / (1 - 0.75) = 0.75 / 0.25 = 3 或 “3 to 1”。
        这意味着事件发生的可能性是事件不发生的可能性的 3倍。

Log(Odds)，它也被称为 Logit 函数，就是取 Odds 的自然对数，范围是 (-∞, +∞)。
    Log(Odds) = ln(p / (1 - p))
逻辑回归模型的形式是：
    Log(Odds) = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ
    左边 (Log(Odds))：取值范围是 (-∞, +∞)。
    右边：一个标准的线性组合，取值范围也是 (-∞, +∞)。

16.优势比（Odds Ratios，OR）：
    Odds Ratio (OR) 就是两个 Odds 的比值，它用来比较一个事件在两个不同组别中发生的相对可能性，值域是 [0, +∞)。
    OR = (Odds in Group A) / (Odds in Group B)
    例：假设我们研究吸烟（暴露因素）与肺癌（疾病）的关系。计算OR
        OR = 1：没有关联。
            两个组的Odds相等。暴露因素（吸烟）与疾病（肺癌）之间没有关系。
            Odds₁ = Odds₀ → OR = 1
        OR > 1：正相关。
            这意味着暴露组的Odds大于参照组的Odds。暴露因素可能会增加事件发生的可能性。
            在我们的例子中，OR = 6.0。
            解读： 吸烟者罹患肺癌的 Odds 是非吸烟者的 6倍。吸烟与肺癌呈正相关。
        OR < 1：负相关/保护作用。
            这意味着暴露组的Odds小于参照组的Odds。暴露因素可能会减少（保护）事件发生的可能性。
            例子： 如果研究的是“锻炼”和“心脏病”，可能会得到 OR = 0.5。
            解读： 锻炼者患心脏病的 Odds 是不锻炼者的 0.5倍（即一半）。锻炼是一个保护因素。


Log(Odds Ratio)：
    公式： Log(OR) = ln(OR)
    值域： (-∞, +∞)，并以 0 为中心。
    Log(OR) = 0：没有关联。（因为 ln(1) = 0）
    Log(OR) > 0：正相关。
    Log(OR) < 0：负相关/保护作用。


17.卡方检验（Chi-square test）：
    Chi-square test（χ²检验）是一种用于分类变量（categorical data）的统计检验方法，用来判断观察到的频数与期望频数之间的差异是否显著。
    换句话说，它可以帮你回答：“我看到的数据分布和我预期的分布有显著差异吗？”
    前提条件：列联表中不能有超过20%的格子其期望频数小于5，并且不能有任何格子的期望频数小于1。
    常见类型：
        1.独立性检验（Chi-square test of independence）
            用于判断两个分类变量是否独立
            例如：性别（男/女）和是否喜欢咖啡（是/否）有没有关系
        数据表现为 列联表（contingency table）
        2.适合度检验（Chi-square goodness-of-fit test）
            用于判断一个分类变量的分布是否符合预期分布
            例如：掷骰子 60 次，观察到 1~6 点的频数是否接近均匀分布
    卡方公式：详见5.
    自由度 (df)：
        适合度检验：
            df=k−1                      k = 变量可能的类别数
        独立性检验（列联表 r×c）：
            df=(r−1)×(c−1)              (行数-1) × (列数-1)
    原假设和判断：
        原假设 H0：观察频数与期望频数没有显著差异
        备择假设 H1：观察频数与期望频数有显著差异
        通过 χ² 值和自由度查表，得到 p-value
        如果 p < 0.05 → 拒绝 H0，说明差异显著
    期望频数公式（独立性检验）：
        Eij = 行总计×列总计/总样本数
    总结：
        χ² 值大 → 观察值和期望值差异大 → 两个变量可能不独立，有关系
        χ² 值小 → 观察值和期望值差异小 → 两个变量可能独立，无关

18.费希尔精确检验（Fisher's Exact Test）：
    Fisher's Exact Test 是一种用于分析分类变量（通常是二分变量）之间关联性的统计显著性检验。它特别适用于当你的数据样本量很小，或者列联表中存在期望频数很低（例如，小于5）的情况。
    它的核心思想是：在行列合计（边际总数）固定不变的假设下，计算获得当前观察到的数据格局、以及所有更极端格局的精确概率。
    公式：
        P = {(a+b)!(c+d)!(a+c)!(b+d)!}/a!b!c!d!N!
        其中 a, b, c, d 是四个格子的频数，N 是总样本量。! 表示阶乘。
    例：
             康复	  未康复	  行合计
    服药组	a = 9	  b = 2	   11
    安慰剂组	c = 3	  d = 6	   9
    列合计	  12	    8	   20 (总计)

    算当前格局 (a=9) 的概率：P_current ≈ 0.0325
    比 a=9 更极端的情况是 a=10 和 a=11。
    a=10 的格局是： a=10, b=1, c=2, d=7
    a=11 的格局是： a=11, b=0, c=1, d=8
    计算这些极端格局的概率：
    P(a=10) ≈ 0.0037
    P(a=11) ≈ 0.0001
    p值 = P(当前或更极端) = P(a=9) + P(a=10) + P(a=11) ≈ 0.0325 + 0.0037 + 0.0001 ≈ 0.0363
    结论： 因为 p值 = 0.0363 < 0.05，我们可以在显著性水平0.05上拒绝原假设，认为服药和康复之间存在显著关联，即药是有效的。


19.瓦尔德检验（Wald Test）：
    Wald检验的核心思想是：评估一个模型的某个（或某几个）参数估计值（β）是否与某个特定的假设值（通常是零）有“显著”的差异。
    它是一种参数显著性检验，用于回答：“这个变量（的特征）对模型真的重要吗？”
    对于单个参数（最常见的情况）：
        W = (β̂' - β₀) / SE(β̂)
        β̂'： 是模型参数的估计值（例如，从线性回归或逻辑回归中得到的系数）。
        β₀： 是零假设中参数的假设值。在绝大多数情况下，我们想检验“参数是否不等于0”，所以 β₀ = 0。
        SE(β̂)： 是参数估计值的标准误。它衡量了β̂的估计有多精确（不确定性）。标准误越小，估计越精确。
        这个统计量（W）在大样本下近似服从标准正态分布。因此，我们可以根据标准正态分布表来计算p值。
        通常我们会将其平方：
        W² = [(β̂ - β₀) / SE(β̂)]²
        这个平方后的统计量在大样本下近似服从卡方分布，自由度为1。




